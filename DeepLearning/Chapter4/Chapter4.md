# Chapter 4 신경망 학습
**학습**이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻한다. 이번 장에서는 신경망이 학습할 수 있도록 해주는 **지표**인 **손실 함수**를 배운다. 이 손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표이다. 이번 장에서는 손실 함수의 값을 가급적 작게 만드는 기법으로, 함수의 기울기를 활용하는 경사법을 소개한다.      

## 4.1 데이터에서 학습
신경망의 특징은 데이터를 보고 학습할 수 있다. 데이터에서 학습한다는 것은 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 뜻이다. 파이썬으로 MNIST 데이터셋의 손글씨 숫자를 학습하는 코드를 구현해보자.  
`2장의 퍼셉트론도 직선으로 분리할 수 있는(선형 분리 가능) 문제라면 데이터로부터 자동으로 학습 가능하다(퍼셉트론 수렴 정리). 하지만 비선형 분리 문제는 자동으로 학습할 수 없다.`    

### 4.1.1 데이터 주도 학습
기계학습에서 모아진 데이터로부터 규칙을 찾아내는 역할을 '기계'가 담당한다. 이미지에서 특징을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있는데, 여기서 말하는 특징은 입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기를 가리킨다. 이미지의 특징은 보통 벡터로 기술하고, 컴퓨터 비전 분야에서는 SIF, SURF, HOG 등의 특징을 많이 사용한다. 이런 특징을 사용하여 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습할 수 있다. 다만, 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 '사람'이 설계하는 것임에 주의해야 한다.  
![4-1](../Images/4_1.png)  
[그림 4-1]    
그림과 같이 신경망은 이미지를 '있는 그대로' 학습한다. 두 번째 접근 방식(특징과 기계학습)에서는 특징을 사람이 설계했지만, 신경망은 이미지에 포함된 중요한 특징까지도 '기계'가 스스로 학습할 것이다. 신경망은 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'end-to-end'로 학습할 수 있다.  
`딥러닝을 **종단간 기계학습**이라고도 한다. 여기서 종단간은 '처음부터 끝까지'라는 의미로, 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻는다는 뜻을 담고 있다.`    

### 4.1.2 훈련 데이터와 시험 데이터(trainig data, test data)
기계학습 문제는 데이터를 **훈련 데이터**와 **시험 데이터**로 나눠 학습과 실험을 수행하는 것이 일반적이다. 우선 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾고, 시험 데이터를 사용하여 앞서 훈련한 모델의 실력을 평가한다. 데이터셋 하나로만 매개변수의 학습과 평가를 수행하면 올바른 평가가 될 수 없다. 참고로 한 데이터셋에만 지나치게 최적화된 상태를 **오버피팅**이라고 한다. 오버피팅 피하기는 기계학습의 중요한 과제이기도 하다.    

## 4.2 손실 함수(loss function)
신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현한다. 그리고 그 지표를 기준으로 최적의 가중치 매개변수의 값을 탐색한다. 이 손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 평균 제곱 오차(mean square error)와 교차 엔트로피 오차(cross entropy error) 를 사용한다.  
`손실 함수는 신경망 성능의 '나쁨'을 나타내는 지표로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타낸다. 손실 함수에 마이너스만 곱하면 '얼마나 나쁘지 않나', 즉 '얼마나 좋으냐'라는 지표로 변신한다.`    

### 4.2.1 평균 제곱 오차(mean squared error, MSE)
![4-2](../Images/4_2.png)  
[그림 4-2]    
가장 많이 쓰이는 손실 함수는 **평균 제곱 오차**이다. 이 식은 ( 신경망의 출력(신경망이 추정한 값) - 정답 레이블 )^2의 합을 구하고, 차원의 개수로 나눠 제곱 오차의 평균을 구해준다.   
```python
def mean_squared_error(y, t) :
    return 0.5 * np.sum((y-t)**2)
``` 
```python
>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]      # 정답 '2'
>>>
>>> # 예1 : '2'일 확률이 가장 높다고 추정(0.6)
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
>>> mean_squared_error(np.array(y), np.array(t))
0.09750000000000003
>>>
>>> # 예2 : '7'일 확률이 가장 높다고 추정(0.6)
>>> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] 
>>> mean_squared_error(np.array(y), np.array(t))
0.5975
```  
첫 번째 예는 정답이 '2'고 신경망의 출력도 '2'에서 가장 높은 경우이다. 한편, 두 번째 예에서는 정답은 똑같이 '2'지만, 신경망의 출력은 '7'에서 가장 높다. 이 실험의 결과로 첫 번째 예의 손실 함수 쪽 출력이 작으면 정답 레이블과의 오차도 작은 것을 알 수 있다. 즉, 평균 제곱 오차를 기준으로는 첫 번째 추정 결과가 오차가 더 작으니 정답에 더 가까울 것으로 판단할 수 있다.    

### 4.2.2 교차 엔트로피 오차 (cross entropy error)
![4-3](../Images/4_3.png)  
[그림 4-3]    
또 다른 손실 함수로서 **교차 엔트로피 오차**도 자주 이용한다. 여기에서 log는 밑이 e인 자연로그이다. 왼쪽 yi는 정답 레이블, log 뒤에 오는 yi는 신경망의 출력이다. 또 첫 번째 yi는 정답에 해당하는 인덱스의 원소만 1이고, 나머지는 0이다(원-핫-인코딩). 그래서 실질적으로 정답일 때의 추정(yi가 1일 때의 yi)의 자연로그를 계산하는 식이 된다. 예를 들어, 정답 레이블은 '2'가 정답이라 하고, 이때의 신경망 출력이 0.6이라면 교차 엔트로피 오차는 -log 0.6 = 0.51이 된다. 즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.  
y = logx 그래프는 x가 1일 때 y는 0이 되고, x가 0에 가까워질수록 y의 값은 점점 작아진다. [그림 4-3]에 해당하는 식도 마찬가지로 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 된다. 반대로 정답일 때의 출력이 작아질수록 오차는 커진다.     
```python
def cross_entropy_error(y, t) :
    # np.log() 함수에 0을 입력하면 -inf가 되어 계산을 진행할 수 없게 됨
    delta = 1e-7    
    return -np.sum(t * np.log(y + delta))
```
```python
>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
>>> cross_entropy_error(np.array(y), np.array(t))
0.510825457099338
>>>
>>> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
>>> cross_entropy_error(np.array(y), np.array(t))
2.302584092994546
```
첫 번째 예는 정답일 때의 출력이 0.6인 경우로, 이때의 교차 엔트로피 오차는 약 0.51이다. 그다음은 정답일 때의 출력이 (더 낮은) 0.1인 경우로, 이떄의 교차 엔트로피 오차는 무려 2.3이다. 즉, 결과(오차 값)가 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단한 것으로, 앞서 평균 제곱 오차의 판단과 일치한다.    

### 4.2.3 미니배치 학습





