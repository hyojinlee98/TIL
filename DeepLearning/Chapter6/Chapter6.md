# Chapter 6 학습 관련 기술들

이번 장에서 다룰 주제는 가중치 매개변수의 최적값을 탐색하는 최적화 방법, 가중치 매개변수 초깃값, 하이퍼파라미터 설정 방법 등 신경망 학습에서 중요한 주제이다. 오버피팅의 대응책인 가중치 감소와 드롭아웃 등의 정규화 방법도 간략히 설명하고 구현해보자. 마지막으로 최근 많은 연구에서 사용하는 배치 정규화도 짧게 알아보자. 이번 장에서 설명하는 기법을 이용하면 신경망(딥러닝) 학습의 효율과 정확도를 높일 수 있다.  

## 6.1 매개변수 갱신
신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이다. 이는 곧 매개변수의 최적값을 찾는 문제이며, 이러한 문제를 푸는 것을 **최적화(potimization)**라 한다. 매개변수 공간은 매우 넓고 복잡해서 신경망 최적화는 굉장히 어려운 문제다. 수식을 풀어 순식간에 최솟값을 구하는 방법 같은 것은 없다. 게다가 심층 신경망에서는 매개변수의 수가 엄청나게 많아져서 사태는 더욱 심각해진다.  

우리는 지금까지 최적의 매개변수 값을 찾는 단서로 매개변수의 기울기(미분)를 이용했다. 매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 몇 번이고 반복해서 점점 최적의 값에 다가갔다. 이것이 **확률적 경사 하강법(SGD)**이란 단순한 방법인데, 매개변수 공간을 무작정 찾는 것보다 똑똑한 방법이다. SGD는 단순하지만, 문제에 따라 SGD보다 똑똑한 방법도 있다. SGD의 단점을 알아본 후 SGD와는 다른 최적화 기법을 알아보자.  

### 6.1.2 확률적 경사 하강법(SGD)
SGD는 수식으로 다음과 같이 쓸 수 있다.  

![6.1](../Images/e_6.1.png)  
[식 6.1]  

여기에서 W는 갱신할 가중치 매개변수고 ∂L/∂W은 W에 대한 손실 함수의 기울기다. η는 학습률(learning_rate)을 의미하는데, 실제로는 0.01이나 0.001과 같은 값을 미리 정해서 사용한다. 
